import streamlit as st
import requests
import json
import base64
import os

# 1. í˜ì´ì§€ ê¸°ë³¸ ì„¤ì •
st.set_page_config(page_title="ì¤‘Â·ê³ ë“± í•™ì  íŒŒíŠ¸ë„ˆ", page_icon="ğŸ«", layout="centered")
st.title("ğŸ« í•™ì  íŒŒíŠ¸ë„ˆ (ìë™ ì—°ê²° ëª¨ë“œ)")

# 2. API í‚¤ í™•ì¸
if "GEMINI_API_KEY" not in st.secrets:
    st.error("ğŸš¨ Secretsì— GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

API_KEY = st.secrets["GEMINI_API_KEY"]

# 3. [í•µì‹¬] ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ìë™ íƒìƒ‰ í•¨ìˆ˜
@st.cache_resource
def find_working_model(api_key):
    # êµ¬ê¸€ì— "ë‚´ í‚¤ë¡œ ì“¸ ìˆ˜ ìˆëŠ” ëª¨ë¸ ëª©ë¡ ë³´ì—¬ì¤˜" ìš”ì²­
    url = f"https://generativelanguage.googleapis.com/v1beta/models?key={api_key}"
    try:
        response = requests.get(url)
        if response.status_code != 200:
            return None, response.json() # ì—ëŸ¬ ë°œìƒ
        
        data = response.json()
        # 'generateContent' ê¸°ëŠ¥ì„ ì§€ì›í•˜ëŠ” ëª¨ë¸ë§Œ í•„í„°ë§
        available_models = [
            m['name'] for m in data.get('models', [])
            if 'generateContent' in m.get('supportedGenerationMethods', [])
        ]
        
        # ìš°ì„ ìˆœìœ„: 1.5 Flash -> 1.5 Pro -> 1.0 Pro ìˆœì„œë¡œ ì°¾ìŒ
        preferred_order = ['models/gemini-1.5-flash', 'models/gemini-1.5-pro', 'models/gemini-pro']
        
        for model in preferred_order:
            if model in available_models:
                return model, available_models
        
        # ìš°ì„ ìˆœìœ„ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì•„ë¬´ê±°ë‚˜ ì²« ë²ˆì§¸ ê²ƒ ì„ íƒ
        if available_models:
            return available_models[0], available_models
            
        return None, "ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. (API ì„¤ì • í™•ì¸ í•„ìš”)"
        
    except Exception as e:
        return None, str(e)

# 4. ëª¨ë¸ íƒìƒ‰ ì‹¤í–‰
with st.spinner("ğŸ”‘ API í‚¤ ê¶Œí•œ ë° ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸ í™•ì¸ ì¤‘..."):
    selected_model, debug_info = find_working_model(API_KEY)

# 5. ì§„ë‹¨ ê²°ê³¼ ì²˜ë¦¬ (ì—¬ê¸°ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤!)
if selected_model:
    st.success(f"âœ… ì—°ê²° ì„±ê³µ! í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸: **{selected_model}**")
else:
    st.error("ğŸš« ì¹˜ëª…ì  ì˜¤ë¥˜: API í‚¤ëŠ” ë§ì§€ë§Œ, ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # ìƒì„¸ ì›ì¸ ë¶„ì„
    if isinstance(debug_info, dict) and 'error' in debug_info:
        err_msg = debug_info['error'].get('message', '')
        st.error(f"êµ¬ê¸€ ì„œë²„ ë©”ì‹œì§€: {err_msg}")
        
        if "API has not been used" in err_msg or "not enabled" in err_msg:
            st.warning("""
            [í•´ê²° ë°©ë²•]
            ì„ ìƒë‹˜, êµ¬ê¸€ í´ë¼ìš°ë“œ ì½˜ì†”ì—ì„œ **'Generative Language API'**ê°€ êº¼ì ¸ ìˆìŠµë‹ˆë‹¤.
            1. Google Cloud Console ì ‘ì†
            2. ê²€ìƒ‰ì°½ì— 'Generative Language API' ê²€ìƒ‰
            3. **'ENABLE(ì‚¬ìš©)'** ë²„íŠ¼ í´ë¦­
            4. 5ë¶„ ë’¤ ë‹¤ì‹œ ì ‘ì†í•˜ë©´ í•´ê²°ë©ë‹ˆë‹¤.
            """)
    else:
        st.code(debug_info)
    st.stop()

# 6. ì±„íŒ… ë¡œì§ (ì„ íƒëœ ëª¨ë¸ë¡œ ëŒ€í™”)
SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ í•™êµ í•™ì  ì—…ë¬´ë¥¼ ì§€ì›í•˜ëŠ” ì „ë¬¸ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ë‹µë³€ ëì—ëŠ” ë°˜ë“œì‹œ ê·¼ê±°(ë¬¸ì„œëª…, í˜ì´ì§€)ë¥¼ ëª…ì‹œí•´ì•¼ í•©ë‹ˆë‹¤.
"""

def call_gemini(prompt, pdf_files, model_name):
    url = f"https://generativelanguage.googleapis.com/v1beta/{model_name}:generateContent?key={API_KEY}"
    
    parts = [{"text": SYSTEM_PROMPT}, {"text": f"ì§ˆë¬¸: {prompt}"}]
    
    # PDF ì²¨ë¶€ (ëª¨ë¸ì´ 1.5 ë²„ì „ì¼ ë•Œë§Œ ê°€ëŠ¥, 1.0ì€ í…ìŠ¤íŠ¸ë§Œ)
    is_vision_model = "1.5" in model_name
    if is_vision_model:
        for pdf_path in pdf_files:
            try:
                with open(pdf_path, "rb") as f:
                    pdf_data = base64.b64encode(f.read()).decode("utf-8")
                    parts.append({
                        "inline_data": {
                            "mime_type": "application/pdf",
                            "data": pdf_data
                        }
                    })
            except:
                pass
    elif pdf_files:
        st.toast("âš ï¸ í˜„ì¬ ì—°ê²°ëœ ëª¨ë¸(Gemini Pro)ì€ PDF ì§ì ‘ ì½ê¸°ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.", icon="â„¹ï¸")

    payload = {"contents": [{"parts": parts}]}
    response = requests.post(url, headers={"Content-Type": "application/json"}, data=json.dumps(payload))
    return response

# PDF íŒŒì¼ ì°¾ê¸°
pdf_files = [f for f in os.listdir('.') if f.lower().endswith('.pdf')]

# ì±„íŒ… UI
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        msg_box = st.empty()
        msg_box.markdown("ë‹µë³€ ìƒì„± ì¤‘... â³")
        
        try:
            res = call_gemini(prompt, pdf_files, selected_model)
            
            if res.status_code == 200:
                answer = res.json()["candidates"][0]["content"]["parts"][0]["text"]
                msg_box.markdown(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})
            else:
                msg_box.error(f"ì˜¤ë¥˜ ë°œìƒ: {res.status_code}")
                msg_box.json(res.json())
        except Exception as e:
            msg_box.error(f"í†µì‹  ì˜¤ë¥˜: {e}")
